{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yiRWhvti1t65"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9xs6SOgzjO1",
        "outputId": "6811794c-8dcd-419f-85a2-5fe42e14818c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Всякое"
      ],
      "metadata": {
        "id": "yiRWhvti1t65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics torchinfo onnxruntime-gpu onnx onnxsim onnxoptimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBT8E9on1lS5",
        "outputId": "226558f1-613e-4248-cb05-20761cd3173c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m678.1/678.1 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from itertools import groupby\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool_ksize=(2, 2)):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                                   nn.LeakyReLU(0.1),\n",
        "                                   nn.BatchNorm2d(out_channels),\n",
        "                                   nn.MaxPool2d(pool_ksize))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, alphabet_len):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(ConvBlock(1, 32),\n",
        "                                               ConvBlock(32, 64, (2, 1)),\n",
        "                                               ConvBlock(64, 64),\n",
        "                                               ConvBlock(64, 128),\n",
        "                                               ConvBlock(128, 256, (2, 1)))\n",
        "        self.lstm1 = nn.LSTM(258, 256, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(256, 256, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(256, alphabet_len+1),\n",
        "                                nn.Softmax(dim=2))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.feature_extractor(x1)[:, :, 0, :]\n",
        "        f1 = torch.permute(f1, (0, 2, 1))\n",
        "\n",
        "        x = torch.cat([f1, x2], dim=2)\n",
        "\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def decode_texts(logits, alphabet):\n",
        "    \"\"\"Decodes CRNN output with given alphabet and whitelist\n",
        "\n",
        "    Args:\n",
        "        logits: np.ndarray, CRNN output\n",
        "        alphabet: str, alphabet CRNN was trained on\n",
        "    Returns:\n",
        "        list of predictions\n",
        "    \"\"\"\n",
        "    best_path_indices = np.argmax(logits, axis=-1)\n",
        "    best_chars_collapsed = [[alphabet[k-1] for k, _ in groupby(e) if k != 0] for e in best_path_indices]\n",
        "    return [''.join(e) for e in best_chars_collapsed]"
      ],
      "metadata": {
        "id": "dLoOHl6Y1nPt"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_loss_log_differentiable_torch(log_logits: torch.FloatTensor, targets: torch.LongTensor,\n",
        "                                      input_lengths: torch.Tensor, target_lengths: torch.Tensor, device,\n",
        "                                      blank_idx=0, dtype_to_use=torch.float32) -> torch.float32:\n",
        "    \"\"\"\n",
        "    log_logits: np.ndarray of shape (B, T, C)\n",
        "    targets: np.ndarray of shape (B, L,)\n",
        "    \"\"\"\n",
        "\n",
        "    B, T = log_logits.shape[0], log_logits.shape[1]\n",
        "    S = 2 * targets.shape[1] + 1\n",
        "\n",
        "    zero = torch.finfo(dtype_to_use).min\n",
        "\n",
        "    # insert blanks between every pair of labels and add them to start and end of the seq\n",
        "    extended_targets = torch.stack([torch.full_like(targets, blank_idx), targets], dim=-1).flatten(start_dim=-2)\n",
        "    extended_targets = torch.cat([extended_targets, torch.full((B, 1), blank_idx, device=device)], dim=-1)\n",
        "    # due to the paper formula for alpha_t(s) we must know where labels repeat and where the blanks are\n",
        "    # in the extended label seq\n",
        "    targets_difference_mask = torch.cat([torch.full((B, 2), False, device=device), extended_targets[:, 2:] != extended_targets[:, :-2]], dim=-1)\n",
        "\n",
        "    # initialize alphas array to keep track of previous alphas\n",
        "    # (also add 2 to the second dim so our s-2 and s-1 vectorized calculations won't get IndexError)\n",
        "    log_alphas = torch.full((B, T, S+2), zero, dtype=dtype_to_use, device=device)\n",
        "\n",
        "    # every accountable prefix starts either with a blank or the first symbol of the target,\n",
        "    # so we initialize alphas in the following way (remember about S+2)\n",
        "    log_alphas[:, 0, 2] = log_logits[:, 0, blank_idx]\n",
        "    log_alphas[:, 0, 3] = log_logits[torch.arange(B), 0, targets[:, 0]]\n",
        "\n",
        "    for t in range(1, T):\n",
        "        # remember we're in log space so log(a*b) = log(a) + log(b)\n",
        "        # here formula must be mathematically reworked.\n",
        "\n",
        "        log_alphas[:, t, 2:] = (torch.gather(log_logits[:, t], -1, extended_targets) +\n",
        "                                torch.logsumexp(torch.stack([log_alphas[:, t-1, 2:], log_alphas[:, t-1, 1: -1],\n",
        "                                                             torch.where(targets_difference_mask,\n",
        "                                                                         log_alphas[:, t-1, :-2], zero)]), dim=0))\n",
        "\n",
        "    temp = torch.gather(log_alphas[np.arange(B), input_lengths-1], -1,\n",
        "                        torch.stack([2 + target_lengths * 2 - 1, 2 + target_lengths * 2], dim=-1))\n",
        "\n",
        "    return -torch.mean(torch.logsumexp(temp, dim=-1))"
      ],
      "metadata": {
        "id": "-uikGvu119e4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, images, abits, labels):\n",
        "        super(OCRDataset, self).__init__()\n",
        "\n",
        "        self.images = images\n",
        "        self.abits = abits\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.FloatTensor(self.images[idx]).unsqueeze(0), torch.FloatTensor(self.abits[idx])), torch.IntTensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "XcA9oCPN-SR_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('/content/drive/MyDrive/CRNN for long fields/common_fields_images.h5') as f:\n",
        "    images = f['images'][:]\n",
        "    additional_bits = f['additional_bit'][:]\n",
        "\n",
        "with open('/content/drive/MyDrive/CRNN for long fields/common_fields_labels.txt', encoding='cp1251') as f:\n",
        "    markup = [e.strip() for e in f.readlines()]\n",
        "\n",
        "\n",
        "def encode_texts(texts):\n",
        "    def _label_to_num(label, alphabet):\n",
        "        label_num = []\n",
        "        for ch in label:\n",
        "            label_num.append(alphabet.find(ch) + 1)\n",
        "        return np.array(label_num)\n",
        "\n",
        "    # alphabet = ''.join(sorted(pd.Series(texts).apply(list).apply(pd.Series).stack().unique()))\n",
        "    alphabet = ''.join(sorted(set(''.join(texts))))\n",
        "\n",
        "    nums = np.zeros([len(texts), max([len(text) for text in texts])], dtype='int64')\n",
        "    for i, text in enumerate(texts):\n",
        "        nums[i][:len(text)] = _label_to_num(text, alphabet)\n",
        "\n",
        "    return nums, alphabet\n",
        "\n",
        "labels_encoded, alphabet = encode_texts(markup)\n",
        "images = images.astype('float64') / 255\n",
        "\n",
        "additional_bits_expanded = np.zeros((len(images), 50, 2))\n",
        "additional_bits_expanded[:, :, additional_bits] = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "train_indices = np.random.choice(np.arange(images.shape[0]), int(images.shape[0]*0.8), replace=False)\n",
        "val_indices = [e for e in np.arange(images.shape[0]) if e not in train_indices]\n",
        "\n",
        "assert len(set(train_indices) & set(val_indices)) == 0\n",
        "assert len(set(train_indices) | set(val_indices)) == images.shape[0]\n",
        "\n",
        "train_imgs = images[train_indices]\n",
        "val_imgs = images[val_indices]\n",
        "\n",
        "train_abits = additional_bits_expanded[train_indices]\n",
        "val_abits = additional_bits_expanded[val_indices]\n",
        "\n",
        "train_labels = labels_encoded[train_indices]\n",
        "val_labels = labels_encoded[val_indices]\n",
        "\n",
        "train_dataset = OCRDataset(train_imgs, train_abits, train_labels)\n",
        "val_dataset = OCRDataset(val_imgs, val_abits, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)"
      ],
      "metadata": {
        "id": "Tb7xtZrW1rAG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.text import CharErrorRate\n",
        "from itertools import groupby\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def validate_model(model, dataloader, device='cpu'):\n",
        "  model.eval()\n",
        "\n",
        "  criterion = ctc_loss_log_differentiable_torch\n",
        "  metric = CharErrorRate()\n",
        "  loss = 0\n",
        "  cer_value = 0\n",
        "  cumtime = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, ((x1, x2), y) in tqdm(enumerate(dataloader)):\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      start = time.time()\n",
        "      y_pred = model(x1, x2)\n",
        "      cumtime += time.time() - start\n",
        "\n",
        "      input_lengths = torch.full((y_pred.shape[0],), y_pred.shape[1]).to(device)\n",
        "      target_lengths = torch.sum(y != 0, axis=1)\n",
        "      loss += criterion(torch.log(y_pred), y, input_lengths, target_lengths, device=device).item()\n",
        "      cer_value += metric(decode_texts(y_pred.detach().cpu().numpy(), alphabet),\n",
        "                        [''.join(alphabet[k-1] for k, _ in groupby(e) if k != 0) for e in y.cpu().numpy().astype(int)]).item()\n",
        "\n",
        "  print()\n",
        "\n",
        "  return cumtime / len(dataloader), loss / len(dataloader), cer_value / len(dataloader)"
      ],
      "metadata": {
        "id": "WBvC8UCM15wJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка модели"
      ],
      "metadata": {
        "id": "3WkU8RpG1vnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = CRNN(len(alphabet))\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Методы компрессии/crnn_common_fields_.pt', map_location=torch.device(device)))\n",
        "summary(model, input_size=[(32, 1, 32, 400), (32, 50, 2)], device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_HjpWxB1w37",
        "outputId": "f581d3ad-b7ac-4a8d-a9e6-58a4f3292d1a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CRNN                                     [32, 50, 46]              --\n",
              "├─Sequential: 1-1                        [32, 256, 1, 50]          --\n",
              "│    └─ConvBlock: 2-1                    [32, 32, 16, 200]         --\n",
              "│    │    └─Sequential: 3-1              [32, 32, 16, 200]         384\n",
              "│    └─ConvBlock: 2-2                    [32, 64, 8, 200]          --\n",
              "│    │    └─Sequential: 3-2              [32, 64, 8, 200]          18,624\n",
              "│    └─ConvBlock: 2-3                    [32, 64, 4, 100]          --\n",
              "│    │    └─Sequential: 3-3              [32, 64, 4, 100]          37,056\n",
              "│    └─ConvBlock: 2-4                    [32, 128, 2, 50]          --\n",
              "│    │    └─Sequential: 3-4              [32, 128, 2, 50]          74,112\n",
              "│    └─ConvBlock: 2-5                    [32, 256, 1, 50]          --\n",
              "│    │    └─Sequential: 3-5              [32, 256, 1, 50]          295,680\n",
              "├─LSTM: 1-2                              [32, 50, 256]             528,384\n",
              "├─LSTM: 1-3                              [32, 50, 256]             526,336\n",
              "├─Sequential: 1-4                        [32, 50, 46]              --\n",
              "│    └─Linear: 2-6                       [32, 50, 46]              11,822\n",
              "│    └─Softmax: 2-7                      [32, 50, 46]              --\n",
              "==========================================================================================\n",
              "Total params: 1,492,398\n",
              "Trainable params: 1,492,398\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 7.49\n",
              "==========================================================================================\n",
              "Input size (MB): 1.65\n",
              "Forward/backward pass size (MB): 413.47\n",
              "Params size (MB): 5.97\n",
              "Estimated Total Size (MB): 421.09\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(zip(['batch_time', 'loss', 'metric'], [round(e, 6) for e in validate_model(model, val_loader, device=device)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bggFBKwM2HHf",
        "outputId": "2211f511-1c6e-4f7b-d33f-aae566a8a235"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26it [00:02,  9.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{'batch_time': 0.005964, 'loss': 0.623181, 'metric': 0.049073}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Конвертация модели в onnx и замеры\n",
        "\n",
        "1. В модели заменил строковое значение padding='same' у сверточных слоев на численное padding=1 (т.к. kernel_size=3, stride=1). Чтобы экспортер отработал корректно\n",
        "2. Пришлось указать dynamic_axes из-за LSTM"
      ],
      "metadata": {
        "id": "srJAtdUh2pNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = next(iter(train_loader))[0]\n",
        "\n",
        "model.to(device)\n",
        "torch.onnx.export(model, (x1[0].unsqueeze(0).to(device), x2[0].unsqueeze(0).to(device)),\n",
        "                  \"/content/drive/MyDrive/Методы компрессии/crnn_common_fields.onnx\",\n",
        "                  input_names=['image_data', 'field_data'], output_names=['output'],\n",
        "                  dynamic_axes={'image_data' : {0 : 'batch_size'},\n",
        "                                'field_data' : {0 : 'batch_size'},\n",
        "                                'output' : {0 : 'batch_size'}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4nDuA798GbH",
        "outputId": "a8393876-db95-494d-b1f6-3b0a543c2785"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:4476: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m onnxoptimizer \"/content/drive/MyDrive/Методы компрессии/crnn_common_fields.onnx\" \"/content/drive/MyDrive/Методы компрессии/crnn_common_fields_opt.onnx\"\n",
        "!onnxsim \"/content/drive/MyDrive/Методы компрессии/crnn_common_fields_opt.onnx\" \"/content/drive/MyDrive/Методы компрессии/crnn_common_fields_opt.onnx\""
      ],
      "metadata": {
        "id": "_X4krU3PPO3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import onnxruntime as ort\n",
        "\n",
        "\n",
        "sess = ort.InferenceSession(\"/content/drive/MyDrive/Методы компрессии/crnn_common_fields_opt.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "\n",
        "y_pred = list()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "output_names = [output.name for output in sess.get_outputs()]\n",
        "\n",
        "for i in range(1, val_imgs.shape[0] // 128 + 2):\n",
        "    y_pred.append(sess.run(output_names, {'image_data': np.expand_dims(val_imgs, 1).astype('float32')[(i-1)*128: i*128],\n",
        "                                          'field_data': val_abits[(i-1)*128: i*128].astype('float32')})[0])\n",
        "time_spent = (time.time()-start) / val_imgs.shape[0]\n",
        "\n",
        "y_pred = np.concatenate(y_pred)\n",
        "\n",
        "input_lengths = torch.full((y_pred.shape[0],), y_pred.shape[1]).to('cpu')\n",
        "val_labels = torch.LongTensor(val_labels).to('cpu')\n",
        "target_lengths = torch.sum(val_labels != 0, axis=1)\n",
        "\n",
        "criterion = ctc_loss_log_differentiable_torch\n",
        "metric = CharErrorRate()\n",
        "\n",
        "loss = criterion(torch.log(torch.FloatTensor(y_pred).to('cpu')), val_labels, input_lengths, target_lengths, device='cpu').item()\n",
        "cer_value = metric(decode_texts(y_pred, alphabet), [''.join(alphabet[k-1] for k, _ in groupby(e) if k != 0) for e in val_labels.cpu().numpy().astype(int)]).item()\n",
        "print(f'sample_time: {round(time_spent, 6)}, loss: {round(loss, 6)}, metric: {round(cer_value, 6)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB4Jdblf90Pg",
        "outputId": "60610130-d6d5-47eb-e9e8-a43255d55c63"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_time: 0.00123, loss: 0.611737, metric: 0.047295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты для inference с PyTorch: {'batch_time': 0.005964, 'loss': 0.623181, 'metric': 0.049073}"
      ],
      "metadata": {
        "id": "Ng315Dn2H6hL"
      }
    }
  ]
}