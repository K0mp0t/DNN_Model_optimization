{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cA93JxN_9fOM"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TWjKNcH8txn",
        "outputId": "f00b0078-afb0-4e79-97f3-0f73f55e9760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Всякое"
      ],
      "metadata": {
        "id": "cA93JxN_9fOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics torchinfo"
      ],
      "metadata": {
        "id": "t6UF6XGh9Nqq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from itertools import groupby\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool_ksize=(2, 2)):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding='same'),\n",
        "                                   nn.LeakyReLU(0.1),\n",
        "                                   nn.BatchNorm2d(out_channels),\n",
        "                                   nn.MaxPool2d(pool_ksize))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, alphabet_len):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(ConvBlock(1, 32),\n",
        "                                               ConvBlock(32, 64, (2, 1)),\n",
        "                                               ConvBlock(64, 64),\n",
        "                                               ConvBlock(64, 128),\n",
        "                                               ConvBlock(128, 256, (2, 1)))\n",
        "        self.lstm1 = nn.LSTM(258, 256, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(256, 256, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(256, alphabet_len+1),\n",
        "                                nn.Softmax(dim=2))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.feature_extractor(x1).squeeze()\n",
        "        f1 = torch.permute(f1, (0, 2, 1))\n",
        "\n",
        "        x = torch.cat([f1, x2], dim=2)\n",
        "\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def decode_texts(logits, alphabet):\n",
        "    \"\"\"Decodes CRNN output with given alphabet and whitelist\n",
        "\n",
        "    Args:\n",
        "        logits: np.ndarray, CRNN output\n",
        "        alphabet: str, alphabet CRNN was trained on\n",
        "    Returns:\n",
        "        list of predictions\n",
        "    \"\"\"\n",
        "    best_path_indices = np.argmax(logits, axis=-1)\n",
        "    best_chars_collapsed = [[alphabet[k-1] for k, _ in groupby(e) if k != 0] for e in best_path_indices]\n",
        "    return [''.join(e) for e in best_chars_collapsed]"
      ],
      "metadata": {
        "id": "Yzd0u8SM9PH8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_loss_log_differentiable_torch(log_logits: torch.FloatTensor, targets: torch.LongTensor,\n",
        "                                      input_lengths: torch.Tensor, target_lengths: torch.Tensor, device,\n",
        "                                      blank_idx=0, dtype_to_use=torch.float32) -> torch.float32:\n",
        "    \"\"\"\n",
        "    log_logits: np.ndarray of shape (B, T, C)\n",
        "    targets: np.ndarray of shape (B, L,)\n",
        "    \"\"\"\n",
        "\n",
        "    B, T = log_logits.shape[0], log_logits.shape[1]\n",
        "    S = 2 * targets.shape[1] + 1\n",
        "\n",
        "    zero = torch.finfo(dtype_to_use).min\n",
        "\n",
        "    # insert blanks between every pair of labels and add them to start and end of the seq\n",
        "    extended_targets = torch.stack([torch.full_like(targets, blank_idx), targets], dim=-1).flatten(start_dim=-2)\n",
        "    extended_targets = torch.cat([extended_targets, torch.full((B, 1), blank_idx, device=device)], dim=-1)\n",
        "    # due to the paper formula for alpha_t(s) we must know where labels repeat and where the blanks are\n",
        "    # in the extended label seq\n",
        "    targets_difference_mask = torch.cat([torch.full((B, 2), False, device=device), extended_targets[:, 2:] != extended_targets[:, :-2]], dim=-1)\n",
        "\n",
        "    # initialize alphas array to keep track of previous alphas\n",
        "    # (also add 2 to the second dim so our s-2 and s-1 vectorized calculations won't get IndexError)\n",
        "    log_alphas = torch.full((B, T, S+2), zero, dtype=dtype_to_use, device=device)\n",
        "\n",
        "    # every accountable prefix starts either with a blank or the first symbol of the target,\n",
        "    # so we initialize alphas in the following way (remember about S+2)\n",
        "    log_alphas[:, 0, 2] = log_logits[:, 0, blank_idx]\n",
        "    log_alphas[:, 0, 3] = log_logits[torch.arange(B), 0, targets[:, 0]]\n",
        "\n",
        "    for t in range(1, T):\n",
        "        # remember we're in log space so log(a*b) = log(a) + log(b)\n",
        "        # here formula must be mathematically reworked.\n",
        "\n",
        "        log_alphas[:, t, 2:] = (torch.gather(log_logits[:, t], -1, extended_targets) +\n",
        "                                torch.logsumexp(torch.stack([log_alphas[:, t-1, 2:], log_alphas[:, t-1, 1: -1],\n",
        "                                                             torch.where(targets_difference_mask,\n",
        "                                                                         log_alphas[:, t-1, :-2], zero)]), dim=0))\n",
        "\n",
        "    temp = torch.gather(log_alphas[np.arange(B), input_lengths-1], -1,\n",
        "                        torch.stack([2 + target_lengths * 2 - 1, 2 + target_lengths * 2], dim=-1))\n",
        "\n",
        "    return -torch.mean(torch.logsumexp(temp, dim=-1))"
      ],
      "metadata": {
        "id": "x6vD6W2z9Qr9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, images, abits, labels):\n",
        "        super(OCRDataset, self).__init__()\n",
        "\n",
        "        self.images = images\n",
        "        self.abits = abits\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.FloatTensor(self.images[idx]).unsqueeze(0), torch.FloatTensor(self.abits[idx])), torch.IntTensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "9ZAUxqxV9VQJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.text import CharErrorRate\n",
        "from itertools import groupby\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def validate_model(model, dataloader, device='cpu'):\n",
        "  model.eval()\n",
        "\n",
        "  criterion = ctc_loss_log_differentiable_torch\n",
        "  metric = CharErrorRate()\n",
        "  loss = 0\n",
        "  cer_value = 0\n",
        "  cumtime = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, ((x1, x2), y) in tqdm(enumerate(dataloader)):\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      start = time.time()\n",
        "      y_pred = model(x1, x2)\n",
        "      cumtime += time.time() - start\n",
        "\n",
        "      input_lengths = torch.full((y_pred.shape[0],), y_pred.shape[1]).to(device)\n",
        "      target_lengths = torch.sum(y != 0, axis=1)\n",
        "      loss += criterion(torch.log(y_pred), y, input_lengths, target_lengths, device=device).item()\n",
        "      cer_value += metric(decode_texts(y_pred.detach().cpu().numpy(), alphabet),\n",
        "                        [''.join(alphabet[k-1] for k, _ in groupby(e) if k != 0) for e in y.cpu().numpy().astype(int)]).item()\n",
        "\n",
        "  print()\n",
        "\n",
        "  return cumtime / len(dataloader), loss / len(dataloader), cer_value / len(dataloader)"
      ],
      "metadata": {
        "id": "kdrt7K_u9mlf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('/content/drive/MyDrive/CRNN for long fields/common_fields_images.h5') as f:\n",
        "    images = f['images'][:]\n",
        "    additional_bits = f['additional_bit'][:]\n",
        "\n",
        "with open('/content/drive/MyDrive/CRNN for long fields/common_fields_labels.txt', encoding='cp1251') as f:\n",
        "    markup = [e.strip() for e in f.readlines()]\n",
        "\n",
        "\n",
        "def encode_texts(texts):\n",
        "    def _label_to_num(label, alphabet):\n",
        "        label_num = []\n",
        "        for ch in label:\n",
        "            label_num.append(alphabet.find(ch) + 1)\n",
        "        return np.array(label_num)\n",
        "\n",
        "    # alphabet = ''.join(sorted(pd.Series(texts).apply(list).apply(pd.Series).stack().unique()))\n",
        "    alphabet = ''.join(sorted(set(''.join(texts))))\n",
        "\n",
        "    nums = np.zeros([len(texts), max([len(text) for text in texts])], dtype='int64')\n",
        "    for i, text in enumerate(texts):\n",
        "        nums[i][:len(text)] = _label_to_num(text, alphabet)\n",
        "\n",
        "    return nums, alphabet\n",
        "\n",
        "labels_encoded, alphabet = encode_texts(markup)\n",
        "images = images.astype('float64') / 255\n",
        "\n",
        "additional_bits_expanded = np.zeros((len(images), 50, 2))\n",
        "additional_bits_expanded[:, :, additional_bits] = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "train_indices = np.random.choice(np.arange(images.shape[0]), int(images.shape[0]*0.8), replace=False)\n",
        "val_indices = [e for e in np.arange(images.shape[0]) if e not in train_indices]\n",
        "\n",
        "assert len(set(train_indices) & set(val_indices)) == 0\n",
        "assert len(set(train_indices) | set(val_indices)) == images.shape[0]\n",
        "\n",
        "train_imgs = images[train_indices]\n",
        "val_imgs = images[val_indices]\n",
        "\n",
        "train_abits = additional_bits_expanded[train_indices]\n",
        "val_abits = additional_bits_expanded[val_indices]\n",
        "\n",
        "train_labels = labels_encoded[train_indices]\n",
        "val_labels = labels_encoded[val_indices]\n",
        "\n",
        "train_dataset = OCRDataset(train_imgs, train_abits, train_labels)\n",
        "val_dataset = OCRDataset(val_imgs, val_abits, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)"
      ],
      "metadata": {
        "id": "sLEpziE-9Wg-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка модели-учителя"
      ],
      "metadata": {
        "id": "GlmGuW2v9cPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "teacher_model = CRNN(len(alphabet))\n",
        "teacher_model.load_state_dict(torch.load('/content/drive/MyDrive/Методы компрессии/crnn_common_fields_.pt', map_location=torch.device(device)))\n",
        "summary(teacher_model, input_size=[(32, 1, 32, 400), (32, 50, 2)], device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2ZSwu059cHu",
        "outputId": "f796314b-7772-44d7-f109-f6ec5c2f1b53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CRNN                                     [32, 50, 46]              --\n",
              "├─Sequential: 1-1                        [32, 256, 1, 50]          --\n",
              "│    └─ConvBlock: 2-1                    [32, 32, 16, 200]         --\n",
              "│    │    └─Sequential: 3-1              [32, 32, 16, 200]         384\n",
              "│    └─ConvBlock: 2-2                    [32, 64, 8, 200]          --\n",
              "│    │    └─Sequential: 3-2              [32, 64, 8, 200]          18,624\n",
              "│    └─ConvBlock: 2-3                    [32, 64, 4, 100]          --\n",
              "│    │    └─Sequential: 3-3              [32, 64, 4, 100]          37,056\n",
              "│    └─ConvBlock: 2-4                    [32, 128, 2, 50]          --\n",
              "│    │    └─Sequential: 3-4              [32, 128, 2, 50]          74,112\n",
              "│    └─ConvBlock: 2-5                    [32, 256, 1, 50]          --\n",
              "│    │    └─Sequential: 3-5              [32, 256, 1, 50]          295,680\n",
              "├─LSTM: 1-2                              [32, 50, 256]             528,384\n",
              "├─LSTM: 1-3                              [32, 50, 256]             526,336\n",
              "├─Sequential: 1-4                        [32, 50, 46]              --\n",
              "│    └─Linear: 2-6                       [32, 50, 46]              11,822\n",
              "│    └─Softmax: 2-7                      [32, 50, 46]              --\n",
              "==========================================================================================\n",
              "Total params: 1,492,398\n",
              "Trainable params: 1,492,398\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 7.49\n",
              "==========================================================================================\n",
              "Input size (MB): 1.65\n",
              "Forward/backward pass size (MB): 413.47\n",
              "Params size (MB): 5.97\n",
              "Estimated Total Size (MB): 421.09\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(zip(['batch_time', 'loss', 'metric'], [round(e, 6) for e in validate_model(teacher_model, val_loader, device=device)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVHAtt5n9pvx",
        "outputId": "0f62239f-d86b-44fa-f6ce-7bc84ab84366"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26it [00:03,  8.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{'batch_time': 0.007023, 'loss': 0.623181, 'metric': 0.049073}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение модели-ученика"
      ],
      "metadata": {
        "id": "glxofVgg9thu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN_light(nn.Module):\n",
        "    def __init__(self, alphabet_len):\n",
        "        super(CRNN_light, self).__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(ConvBlock(1, 16),\n",
        "                                               ConvBlock(16, 32, (2, 1)),\n",
        "                                               ConvBlock(32, 64),\n",
        "                                               ConvBlock(64, 128),\n",
        "                                               ConvBlock(128, 192, (2, 1)))\n",
        "        self.lstm1 = nn.LSTM(194, 192, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(192, 192, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(192, alphabet_len+1),\n",
        "                                nn.Softmax(dim=2))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.feature_extractor(x1).squeeze()\n",
        "        f1 = torch.permute(f1, (0, 2, 1))\n",
        "\n",
        "        x = torch.cat([f1, x2], dim=2)\n",
        "\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "H8GQATCh9vt3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = CRNN_light(len(alphabet))\n",
        "summary(student_model, input_size=[(32, 1, 32, 400), (32, 50, 2)], device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9NXYQOY98eS",
        "outputId": "8661fa0b-4b7b-452e-b3a4-5715523b15e6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CRNN_light                               [32, 50, 46]              --\n",
              "├─Sequential: 1-1                        [32, 192, 1, 50]          --\n",
              "│    └─ConvBlock: 2-1                    [32, 16, 16, 200]         --\n",
              "│    │    └─Sequential: 3-1              [32, 16, 16, 200]         192\n",
              "│    └─ConvBlock: 2-2                    [32, 32, 8, 200]          --\n",
              "│    │    └─Sequential: 3-2              [32, 32, 8, 200]          4,704\n",
              "│    └─ConvBlock: 2-3                    [32, 64, 4, 100]          --\n",
              "│    │    └─Sequential: 3-3              [32, 64, 4, 100]          18,624\n",
              "│    └─ConvBlock: 2-4                    [32, 128, 2, 50]          --\n",
              "│    │    └─Sequential: 3-4              [32, 128, 2, 50]          74,112\n",
              "│    └─ConvBlock: 2-5                    [32, 192, 1, 50]          --\n",
              "│    │    └─Sequential: 3-5              [32, 192, 1, 50]          221,760\n",
              "├─LSTM: 1-2                              [32, 50, 192]             297,984\n",
              "├─LSTM: 1-3                              [32, 50, 192]             296,448\n",
              "├─Sequential: 1-4                        [32, 50, 46]              --\n",
              "│    └─Linear: 2-6                       [32, 50, 46]              8,878\n",
              "│    └─Softmax: 2-7                      [32, 50, 46]              --\n",
              "==========================================================================================\n",
              "Total params: 922,702\n",
              "Trainable params: 922,702\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 4.09\n",
              "==========================================================================================\n",
              "Input size (MB): 1.65\n",
              "Forward/backward pass size (MB): 251.26\n",
              "Params size (MB): 3.69\n",
              "Estimated Total Size (MB): 256.61\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distilation train loop v1 (с декодированием soft targets)\n",
        "\n",
        "Тут пока не сильно ясно: CRNN возвращает логиты, для который при сравнении с таргетом CTCLoss вычисляет вероятность декодирования в верную последовательность, соответственно, логиты модели-учителя сперва нужно декодировать и только потом передавать в CTCLoss.\n",
        "\n",
        "Возможно, получится корретно обучаться и сравнивая \"сырые\" логиты.\n",
        "\n",
        "Ну и не стоит забывать, что мне хватило мозгов использовать свой CTCLoss вместо имплементации из PyTorch -_-"
      ],
      "metadata": {
        "id": "sUHqH3t_-NQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "student_model.to(device)\n",
        "student_model.train()\n",
        "\n",
        "teacher_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.NAdam(student_model.parameters(), lr=1e-3)\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, patience=4, min_lr=1e-5, factor=0.5)\n",
        "metric = CharErrorRate()\n",
        "\n",
        "criterion = ctc_loss_log_differentiable_torch\n",
        "\n",
        "alpha = 0.8\n",
        "epochs = 1000\n",
        "early_stopping_patience = 10\n",
        "val_loss_history = list()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  train_cer = 0\n",
        "  val_cer = 0\n",
        "\n",
        "  for i, ((x1, x2), y) in enumerate(train_loader):\n",
        "    x1 = x1.to(device)\n",
        "    x2 = x2.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = student_model(x1, x2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      soft_targets = teacher_model(x1, x2).argmax(dim=-1)\n",
        "      soft_targets_ = torch.zeros_like(soft_targets)\n",
        "      for j, s in enumerate(soft_targets):\n",
        "        soft_targets_[j, :sum(s != 0)] = s[s != 0]\n",
        "      soft_targets = soft_targets_\n",
        "\n",
        "    input_lengths = torch.full((y_pred.shape[0],), y_pred.shape[1]).to(device)\n",
        "    hard_target_lengths = torch.sum(y != 0, axis=1)\n",
        "    soft_target_lengths = torch.sum(soft_targets != 0, axis=1)\n",
        "\n",
        "    hard_loss = criterion(torch.log(y_pred), y, input_lengths, hard_target_lengths, device=device)\n",
        "    soft_loss = criterion(torch.log(y_pred), soft_targets, input_lengths, soft_target_lengths, device=device)\n",
        "    loss = alpha*hard_loss + (1-alpha)*soft_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    train_cer += metric(decode_texts(y_pred.detach().cpu().numpy(), alphabet),\n",
        "                        [''.join(alphabet[k-1] for k, _ in groupby(e) if k != 0) for e in y.cpu().numpy().astype(int)]).item()\n",
        "\n",
        "    print(f'\\rEpoch {epoch}, {i+1}/{len(train_loader)}, loss: {round(train_loss/(i+1), 6)}, cer: {round(train_cer/(i+1), 6)}', end='')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, ((x1, x2), y) in enumerate(val_loader):\n",
        "      x1 = x1.to(device)\n",
        "      x2 = x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = student_model(x1, x2)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        soft_targets = teacher_model(x1, x2).argmax(dim=-1)\n",
        "        soft_targets_ = torch.zeros_like(soft_targets)\n",
        "        for j, s in enumerate(soft_targets):\n",
        "          soft_targets_[j, :sum(s != 0)] = s[s != 0]\n",
        "        soft_targets = soft_targets_\n",
        "\n",
        "      input_lengths = torch.full((y_pred.shape[0],), y_pred.shape[1]).to(device)\n",
        "      hard_target_lengths = torch.sum(y != 0, axis=1)\n",
        "      soft_target_lengths = torch.sum(soft_targets != 0, axis=1)\n",
        "\n",
        "      hard_loss = criterion(torch.log(y_pred), y, input_lengths, hard_target_lengths, device=device)\n",
        "      soft_loss = criterion(torch.log(y_pred), soft_targets, input_lengths, soft_target_lengths, device=device)\n",
        "      loss = alpha*hard_loss + (1-alpha)*soft_loss\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      val_cer += metric(decode_texts(y_pred.detach().cpu().numpy(), alphabet),\n",
        "                          [''.join(alphabet[k-1] for k, _ in groupby(e) if k != 0) for e in y.cpu().numpy().astype(int)]).item()\n",
        "\n",
        "  print(f' val_loss: {round(val_loss/len(val_loader), 6)}, val_cer: {round(val_cer/len(val_loader), 6)}')\n",
        "\n",
        "  lr_scheduler.step(val_loss/len(val_loader))\n",
        "  val_loss_history.append(val_loss/len(val_loader))\n",
        "\n",
        "  if min(val_loss_history) < min(val_loss_history[-early_stopping_patience:]):\n",
        "    break"
      ],
      "metadata": {
        "id": "TxN6q4v6-Pta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сравнение моделей"
      ],
      "metadata": {
        "id": "HwaVprXBEKEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(zip(['batch_time', 'loss', 'metric'], [round(e, 6) for e in validate_model(teacher_model, val_loader, device=device)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjP4Lc7yELPr",
        "outputId": "185b2244-6785-4d81-e0fb-d43d8102f5ea"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26it [00:01, 15.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{'batch_time': 0.002948, 'loss': 0.623181, 'metric': 0.049073}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(zip(['batch_time', 'loss', 'metric'], [round(e, 6) for e in validate_model(student_model, val_loader, device=device)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKekMUKsEPUa",
        "outputId": "12b6f2dd-779f-494b-d86e-67785e106d86"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26it [00:02, 11.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{'batch_time': 0.002454, 'loss': 1.161216, 'metric': 0.054938}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "3EHGyG3RF8Ws"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RBIhTlXF-7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}